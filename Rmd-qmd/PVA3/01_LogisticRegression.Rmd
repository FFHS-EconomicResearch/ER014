---
title: "Data Science for Business"
subtitle: "Logistische Regression"
author: "Prof. Dr. J√∂rg Schoder"
institute: "FFHS" 
date: "`r Sys.Date()`"
bibliography: ../../lit/my_bib.bib
reference-section-title: Quellenverzeichnis
output:
  xaringan::moon_reader:
    self_contained: true
    css: 
         - default
         - ../../css/ffhs-theme_js.css
         - xaringan-themer.css
    includes:
      after_body: ../../css/insert-logo.html
    lib_dir: ../../libs
    nature:
      slideNumberFormat: "%current%/%total%"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    seal: false
    

    
---
class: title-slide

```{r xaringan-themer, include=FALSE}
library(xaringanthemer)
style_xaringan(text_color = "#d50006",inverse_text_color = "#FFFFFF",inverse_background_color = "#d50006", title_slide_background_color = "#d50006",header_background_color = "#d50006",header_color = "#FFFFFF",header_h1_font_size = "32px",
  header_h2_font_size = "26px",link_color="#502479",
  header_h3_font_size = "20px",text_slide_number_color = "#d50006",text_slide_number_font_size = "0.5em")
```

```{r xaringanExtra, echo=FALSE}
xaringanExtra::use_progress_bar(color = "#d50006", location = "bottom")
xaringanExtra::use_xaringan_extra(c("tile_view","scribble","panelset","tachyons"))
xaringanExtra::style_panelset_tabs(font_family = "inherit")
#xaringanExtra::use_search(show_icon = TRUE)
#weitere: "share_again","animate_css", "webcam","freezeframe","clipboard","fit_screen","extra-styles" 
xaringanExtra::use_editable(expires = 1)
xaringanExtra::use_freezeframe(trigger = "hover")
FFHSred <- "#d50006"
FFHSpurp <- "#502479"
``` 

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(latex2exp)
library(fontawesome)
#library(emo)
source(xfun::from_root("lit","helper.R"))
library(RefManageR)
BibOptions(check.entries = FALSE, 
           bib.style = "authoryear", 
           style = "markdown",
           dashed = TRUE)
file.name <- system.file("Bib", 
                         "my_bib.bib", 
                         package = "RefManageR")
bib <- ReadBib(xfun::from_root("lit","my_bib.bib"))
```

# ER014 - Data Science & Strategy for Business

## PVA3

### Teil 1: Einf√ºhrung Logistische Regressionsanalyse

 

<br>
<br>
<br>
<br>
<br>
<br>
<br>
### FS 2025
<br>
### Prof. Dr. J√∂rg Schoder
.mycontacts[
`r fa('github')` @FFHS-EconomicResearch
`r fa('linkedin')` @jfschoder
]


---
layout: true

<div class="my-footer"></div>       

<div style="position: absolute;left:400px;bottom:10px;font-size:9px">`r fa('creative-commons')``r rmarkdown::metadata$author`</div>


---
name: agenda
class: left

.blockquote[Agenda]

## Einf√ºhrung in die Logistische Regressionsanalyse

* Vom linearen zum generalisierten Modell: Warum brauchen wir glm()?
  
* Praxisrelevanz der logistischen Regression

* Umsetzung in R


---
class: inverse, center, middle

## Vom linearen zum generalisierten Modell: Warum brauchen wir glm()?

.blockquote[Refresher lineares Modell (`lm()`)]

.blockquote[Generalisiertes lineares Modell (`glm()`)]



---
class: left

.blockquote[Refresher lineares Modell (`lm()`)]

## Einfaches lineares Modell

$$Y = \beta_1 + \beta_2 X + \varepsilon, \quad \varepsilon \sim \mathcal{N}(0, \sigma^2)$$

- Beziehung zwischen \( X \) und \( Y \) sch√§tzen
  - abh√§ngige Variable (AV bzw. $Y$): metrisch, normalverteilt
  - abh√§ngige Variable (UV bzw.  $X$): metrisch oder kategorial
- Umsetzung in `r fa('r-project')`: `lm(Y ~ X, data = my_data)`
- t-Test und ANOVA als Spezialf√§lle

| Verfahren | Modellform                            | UV-Typ         | AV-Typ     |
|----------|----------------------------------------|----------------|------------|
| t-Test   | $Y = \beta_0 + \beta_1 \cdot \mbox{Gruppe}$ | bin√§r         | metrisch   |
| ANOVA    | $Y = \beta_0 + \beta_1 D_1 + \beta_2 D_2$ | kategorial (k>2) | metrisch |

‚û°Ô∏è Regression bzw. LM als m√§chtiger "framework"


---
class: left 

.blockquote[Refresher lineares Modell (`lm()`)]

## Multiple lineare Regression

.panelset[
.panel[.panel-name[Theorie]
**Modell:**

$$y = \beta_{0i} + \beta_{1i} x_{1i} + \beta_{2i} x_{2i} + \cdots + \beta_{ki} x_{k2i} + \varepsilon_i$$
bzw. in Matrixnotation

$$Y = \beta X+\varepsilon$$

‚û°Ô∏è  Entscheidender Mehrwert zur Einfachregression: M√∂glichkeit zur Kontrolle von mehreren Einflussfaktoren (Beispiel Gebrauchtwagen (PVA2): Preis, Alter, Kilometer)

]
.panel[.panel-name[Beispiel]
‚û°Ô∏è  Umsetzung in `r fa('r-project')` mit der `lm()`-Funktion

```{r}
#| echo: true
#| message: false
library(tidyverse)
# Import data -----
my_in_file <- "autos_(StockerUIBK)_20240414.csv"
tbl_autos <- read_csv2(xfun::from_root("data","raw",my_in_file))
# Fit regression model ------
reg_auto <- tbl_autos %>%
                lm(Preis ~ Alter + km,.)
```
]
.panel[.panel-name[Regressionstabelle]
```{r}
#| echo: true
#| message: false
# Results -----
library(moderndive) # f√ºr get_regression_()`-Funktion - tidy-Format
library(gt) # f√ºr Tabellendarstellung mit `gt()`-Funktion
reg_auto %>%
  get_regression_table() %>% 
  gt()
```
]
]


---
class: left

.blockquote[Refresher lineares Modell (`lm()`)]

## Annahmen im linearen Regressionsmodell 

.panelset[
.panel[.panel-name[A-Annahmen...]
### ...zur funktionalen Spezifikation

A1 **Variablenauswahl**: Die Gleichung
$$Y = \beta X+\varepsilon$$
enth√§lt alle relevanten (und keine irrelevanten) Regressoren x_i.

A2 üìà **Linearit√§t**: Der (wahre) Zusammenhang zwischen Regressor(en) X und Regressand Y ist linear.


A3 üìå **Parameterkonstanz**. Die Paramter $\beta$ sind f√ºr alle N Beobachtungen $(x_i,y_i)$ konstant. 


]
.panel[.panel-name[B-Annahmen...]
### ...zur St√∂rgr√∂ssen-Spezifikation

B1 üéØ **Erwartungswert**: $\mathbb{E}(\varepsilon_i)=0,~\forall i$

B2 üìä **Homoskedastizit√§t**: Varianz der Fehler ist konstant √ºber alle Werte von X.  
$$\mbox{Var}(\varepsilon_i) = \sigma^2,~\forall i$$
 
B3 üë• **Unabh√§ngigkeit der Fehler** (keine Autokorrelation): Beobachtungen und Fehler sind unabh√§ngig.
$$\mbox{Cov}(\varepsilon_i,\varepsilon_j)=0,~\forall i\neq j$$

B4 üîî **Normalverteilung der Fehler**: Die Residuen (Fehler) sind normalverteilt.
$$\varepsilon \sim \mathcal{N}(0, \sigma^2)$$
]

.panel[.panel-name[C-Annahmen...]
### ...zur Variablen-Spezifikation

C1 üìå **Fixiertheit der Regressoren**: Regressoren sind keine Zufallsvariablen (sie k√∂nnen vielmehr wie in einem Experiment kontrolliert werden).

C2 üîÑ **Keine perfekte Multikollinearit√§t** ($X'X$ invertierbar): Die Regressoren d√ºrfen nicht vollst√§ndig linear abh√§ngig sein.

]
]

.quelle[Vgl. ausf√ºhrlich `r Cite(bib,"vonAuer_oekonometrie_2023")`.]



???


* Verletzung A1: fehlerhafte Modellspezifikation `r fa('circle-right')` verzerrte Sch√§tzer

A3: bspw. keine Strukturbr√ºche

B1 Kontext üìê **Exogenit√§t** (auch: Orthogonalit√§t): Die erkl√§renden Variablen (X) sind strikt exogen und folglich **nicht** mit dem Fehlerterm korreliert:
$$\text{Cov}(X, \varepsilon) = 0$$
Exogenit√§t/Orthogonalit√§t impliziert einen zentrierten Fehler $E(\varepsilon)=0$, aber nicht umgekehrt.


Verletzung B1: Endogenit√§tsproblem `r fa('circle-right')` verzerrte OLS-Koeffizienten

* Verletzung B1:
  * Folgen: 
     - Instabile Sch√§tzer
    - Aufgebl√§hte Standardfehler
    - Geringere Testpower

   * **Diagnose:**
     - Korrelationsmatrix
     - VIF (Variance Inflation Factor): Faustregel: VIF > 5 oder 10 = Warnsignal
  
B2: sonst: Inferenzproblem durch verzerrte Standardfehler
B3: Freiheit von Autokorrelation, sonst: Inferenzproblem durch verzerrte Standarfehler)


B4: Normalit√§tsannahme: wichtig f√ºr Konfidenzintervalle & Tests, aber nicht f√ºr BLUE

* Verletzung von 4 (Unabh√§ngigkeit Fehler): verzerrte Standardfehler - Probleme bei Inferenz (Tests etc.)

* Verletzung 5 (Exogenit√§t Regressoren): verzerrte OLS-Koeffizienten. Regressoren d√ºrfen nicht systematisch mit unbeobachteten Faktoren (Fehlerterm) zusammenh√§ngen.


---
class: left

.blockquote[Refresher lineares Modell (`lm()`)]

## Exkurs: Gauss-Markov-Theorem

.blockquote[
Unter den A-, B-, und C-Annahmen (ohne B4, Normalverteilung) ist der OLS-Sch√§tzer der **beste lineare unverzerrte** Sch√§tzer (BLUE: Best Linear Unbiased Estimator).
]

‚û°Ô∏è Die BLUE-Eigenschaft garantiert, dass der OLS-Sch√§tzer im Rahmen der Annahmen nicht nur im Durchschnitt richtig liegt (unverzerrt ist), sondern auch unter allen linearen Verfahren am wenigsten streut ‚Äì d.h. so pr√§zise wie m√∂glich ist. Dies **ohne** zus√§tzliche Informationen oder **komplexere Methoden zu ben√∂tigen**.

‚úÖ Ist zus√§tzlich die Normalverteilungsannahme (B4) erf√ºllt, dann ist der OLS-Sch√§tzer effizient in der Klasse *s√§mtlicher* (auch nicht-linearer) unverzerrter Sch√§tzer (BUE-Eigenschaft).



---
class: left

.blockquote[Refresher lineares Modell (`lm()`)]

## Regressionsdiagnostik

.panelset[
.panel[.panel-name[Typische Plots]
1. Residuen vs. Fits (Annahmen A2 und B2)
`r fa('circle-right')` Test auf Linearit√§t & Homoskedastizit√§t

2. Normal Q-Q Plot (Annahme B4)
`r fa('circle-right')` Test auf Normalverteilung der Fehler

3. Scale-Location Plot (Annahme B2)
`r fa('circle-right')` Alternativtest auf Homoskedastizit√§t

4. Residuals vs. Leverage (Ausreisser-Diagnostik)
`r fa('circle-right')` Identifikation von einflussreichen Beobachtungen

5. Cook‚Äôs Distance (Ausreisser-Diagnostik)
`r fa('circle-right')` Visualisierung der Einflussst√§rke einzelner F√§lle
]
.panel[.panel-name[Tests]

* Homoskedastizit√§t (B2) mit `car::ncvTest()`, `lmtest::bptest()`

* Autokorrelation (B3) mit `car::durbinWatsonTest()`

* Normalverteilung der Residuen (B4) mit `shapiro.test()`
]
.panel[.panel-name[Umsetzung in R]
```{r}
#| echo: true
#| out-width: "40%"
#| fig-align: "center"
library(ggfortify)
autoplot(reg_auto) + autoplot(reg_auto, which = 4) 
```

]
.panel[.panel-name[Multikollinearit√§t]

* Korrelation der Regressoren `r fa('circle-right')` hoher Wert als Anfangsverdacht; aber keine hinreichende Bedingung!


* Varianzinflation (Variance Inflation Factor, VIF)
```{r}
car::vif(reg_auto)
```
  * Faustregel: 
    * VIF < 5: unproblematisch
    * VIF > 5: Vorsicht geboten
    * VIF > 10: Problematisch

]
]


???

* NCV: Non-constant-Variance Test
* bptest: Breusch-Pagan-Test

---
class: left

.blockquote[Refresher lineares Modell (`lm()`)]

## Exkurs: Multikollinearit√§t

Bei Verstoss gegen Annahme C2:

* Variablenauswahl
  * Theoriegeleitete Auswahl statt rein datengetriebener Ansatz
  * Stufenweise Regression mit AIC/BIC-Kriterien

* Dimensionsreduktion
  * Hauptkomponentenanalyse (PCA) vor der Modellierung
  * Faktoranalyse zur Bildung von latenten Konstrukten

* Regularisierung 
  * Ridge Regression (L2) bspw. 
  * LASSO (L1) f√ºr Variablenselektion

???

Regularisierung bspw. mit `glmnet()`-Funktion


---
class: left

.blockquote[Generalisiertes lineares Modell (`glm()`)]

## Wenn `lm()` an Grenzen st√∂sst

.panelset[
.panel[.panel-name[Problem]
‚ö†Ô∏è Probleme bei nicht-metrischen abh√§ngigen Variablen $Y$

üîç Spezialfall $Y$ **bin√§r** `r fa('circle-right')` keine Normalverteilung oder konstante Varianz m√∂glich
  * Residuen nicht sinnvoll interpretierbar  
  * Homoskedastizit√§t & Normalverteilung **prinzipiell verletzt**

üëâ uns interessiert: Wie wahrscheinlich ist das Ereignis $Y = 1$ bei gegebenem $X$?
  * Modell muss **Vorhersagen in $[0,1]$** machen, weil...
  * ...Wahrscheinlichkeiten per Definition zwischen 0 und 1 liegen
]
.panel[.panel-name[Visualisierung]
```{r}
#| echo: false
#| message: false
#| warning: false
#| out.width: "40%"
#| fig.align: "center"
# Pakete laden
library(tidyverse)
library(broom)

# Simulierte Daten: Alter und Kaufwahrscheinlichkeit
set.seed(42)
n <- 100
alter <- seq(15, 75, length.out = n)
true_p <- 1 / (1 + exp(-(-6 + 0.1 * alter)))  # logistische Beziehung
kauf <- rbinom(n, size = 1, prob = true_p)

# Daten-Frame
df <- tibble(alter, kauf)

# Modelle
mod_lm <- lm(kauf ~ alter, data = df)
mod_glm <- glm(kauf ~ alter, family = binomial, data = df)

# Vorhersagen mit tidy/augment
pred_lm <- augment(mod_lm) %>%
  select(alter, pred_lm = .fitted)

pred_glm <- augment(mod_glm, type.predict = "response") %>%
  select(alter, pred_glm = .fitted)

# Zusammenf√ºhren
plot_data <- df %>%
  left_join(pred_lm, by = "alter") %>%
  left_join(pred_glm, by = "alter")

# Plot
ggplot(plot_data, aes(x = alter, y = kauf)) +
  geom_point(alpha = 0.5, color = "gray40") +
  geom_line(aes(y = pred_lm), color = "red", size = 1.2) +
#  geom_line(aes(y = pred_glm), color = "blue", size = 1.2) +
  labs(
#    title = "Problem der linearen Linkfunktion bei bin√§rer AV",
    subtitle = "Lineares Modell",
    x = "Alter",
    y = "Kauf (0/1)"
  ) +
  scale_y_continuous(limits = c(-0.1, 1.1), breaks = c(0, 0.5, 1)) +
  theme_minimal(base_size = 14)
```

‚ö†Ô∏è Problem: Ein lineares Modell kann **nicht zul√§ssige** Werte < 0 oder > 1 liefern!
]
]




---
class: left

.blockquote[Generalisiertes lineares Modell (`glm()`)]

## Wenn `lm()` an Grenzen st√∂sst (cont'd)

.panelset[
.panel[.panel-name[L√∂sung]
‚û°Ô∏è Verallgemeinerung des linearen Modells `r fa('circle-right')` **GLM**

```{r}
#| echo: false
#| message: false
#| warning: false
#| out.width: "50%"
#| fig.align: "center"
knitr::include_graphics(xfun::from_root('img','PVA3','GLM_Venn_(Lazic_2015).png'))
```

* GLM verkn√ºpft *linearen* Pr√§diktor ($\eta=\beta_0 + \beta_1\cdot x_1 + \dots + \beta_k\cdot x_k$) und abh√§ngige Variable $Y$ durch eine **Verkn√ºpfungsfunktion (Linkfunktion)**.

* Linkfunktionen transformiert *linearen Pr√§diktor* in eine **Wahrscheinlichkeit** $p$.

* Wir modellieren also $P(Y=1|X)$ als Funktion von $X$.
]
.panel[.panel-name[Visualisierung]
```{r}
#| echo: false
#| message: false
#| warning: false
#| out.width: "35%"
#| fig.align: "center"
# Plot
ggplot(plot_data, aes(x = alter, y = kauf)) +
  geom_point(alpha = 0.5, color = "gray40") +
  geom_line(aes(y = pred_lm), color = "red", size = 1.2) +
  geom_line(aes(y = pred_glm), color = "blue", size = 1.2) +
  labs(
#    title = "Problem der linearen Linkfunktion bei bin√§rer AV",
    subtitle = "Logistische Regression (blau)",
    x = "Alter",
    y = "Kauf (0/1)"
  ) +
  scale_y_continuous(limits = c(-0.1, 1.1), breaks = c(0, 0.5, 1)) +
  theme_minimal(base_size = 14)
```
‚û°Ô∏è Logistische Funktion transformiert linearen Pr√§diktor $\eta$ auf den Wertebereich $[0,1]$

`r fa('exclamation-circle')` transformiert wird $\eta$, nicht $Y$ selbst.

]
]

???

Didaktischer Hinweis zu "Warum p?" 
* Klar formulieren, warum $p$ √ºberhaupt gesch√§tzt werden muss:

* Bei bin√§ren Variablen (0/1) interessiert uns nicht der Wert von $Y$, sondern die Wahrscheinlichkeit, mit der das Ereignis $Y=1$ auftritt - gegeben die erkl√§renden Variablen $X$. 

* Wir modellieren also $P(Y=1|X)$ als Funktion von $X$.

* Diese Argumentation hilft besonders, das Modellverst√§ndnis von
  * **Vorhersage eines Wertes (bei lm)** hin zu 
  * **Vorhersage einer Wahrscheinlichkeit (bei glm)** zu verschieben

* Obs! Es wird nicht die abh√§ngige Variable selbst transformiert!

* **Transformationen**:
Hier wird die abh√§ngige Variable selbst transformiert (z.B. logarithmisch oder hochpotenziell), um die Normalverteilung und Linearit√§t der Beziehung zu erf√ºllen.

* **GLMs:**
Hier wird die Verkn√ºpfungsfunktion verwendet, um die Beziehung zwischen der linearen Pr√§diktion und der erwarteten abh√§ngigen Variable zu modellieren



---
class: left

.blockquote[Generalisiertes lineares Modell (`glm()`)]

## Wahrscheinlichkeiten, Odds und die Logit-Funktion

* Ausgangspunkt Wahrscheinlichkeit: $p =\frac{1}{1+e^{-\eta}}= \frac{1}{1 + e^{-(\beta_0 + \beta_1 x+ \dots + \beta_k\cdot x_k)}}$

* Gegenwahrscheinlichkeit: $1-p=1-\frac{1}{1+e^{-\eta}} = \frac{e^{-\eta}}{1+e^{-\eta}}$


* **Odds**

$$\mbox{Odds}\equiv\frac{p}{1-p}=\frac{\frac{1}{1+e^{-\eta}}}{\frac{e^{-\eta}}{1+e^{-\eta}}}=\frac{1}{e^{-\eta}}=e^\eta$$

* Logarithmieren liefert **Log-Odds** (aka Logit-Funktion)

$$log\big(\frac{p}{1-p}\big)=log(e^\eta)=\eta=\beta_0 + \beta_1 x+ \dots + \beta_k\cdot x_k$$
‚ö†Ô∏è Logit-Funktion als Verkn√ºpfungsfunktion. ‚û°Ô∏è Mit der linearen Gleichung werden die sog. log-Odds gesch√§tzt, nicht die Wahrscheinlichkeit!


???

* Odds: Wahrscheinlichkeit durch Gegenwahrscheinlichkeit. √Ñhnlich aber nicht zu verwexhseln mit Wettquoten. Genau genommen Kehrwert der Wettquoten, wie sie in UK angegeben werden.

* Die lineare Sch√§tzung liefert log-Odds des Zielereignisses, diese k√∂nnen dann durch logistische Transformation (inverse logit) in Wahrscheinlichkeiten √ºberf√ºhrt werden.

$\hat{p} = \frac{1}{1 + e^{-\hat{\eta}}}$

üí¨ **F√ºr den Vortrag**:
Was wir direkt sch√§tzen, sind nicht Wahrscheinlichkeiten, sondern die Log-Odds.
Damit wir Aussagen √ºber Wahrscheinlichkeiten treffen k√∂nnen, m√ºssen wir diese Werte erst nichtlinear transformieren.


* Hinweis Odds:

$$\mbox{Odds}(E)=\frac{x}{y}=\frac{x/(x+y)}{y/(x+y)}$$
impliziert:
$$P(E)=x/(x+y),~\mbox{bzw.}~~P(E^c)=y/(x+y)$$

---
class: left

.blockquote[Generalisiertes lineares Modell (`glm()`)]

## Exkurs: Verkn√ºpfungsfunktionen im GLM

.panelset[
.panel[.panel-name[Linkfunktion]
* Linkfunktion abh√§ngig von der Natur der abh√§ngigen Variablen: 
  * Bin√§rdaten: Logit, Probit 
  * Z√§hldaten (nicht negativ): Log-Funktion (insbesondere bei Poisson-Verteilung)
  * stetige Daten mit positivem Skew (Gamma): Inverser Link 
  * normalverteilte Daten: Identit√§tslink  

* Linkfunktionen f√ºr **bin√§re** abh√§ngige Variablen

| Funktion | Formel                              | Eigenschaften                     |
|----------|-------------------------------------|-----------------------------------|
| **Logit**   | $p = \frac{1}{1 + e^{-\eta}}$    | basiert auf logistischer Verteilung |
| **Probit**  | $p = \Phi(\eta)$                 | basiert auf Normalverteilung        |

]
.panel[.panel-name[Logit vs. Probit]
```{r}
#| echo: false
#| message: false
#| warning: false
#| out.width: "55%"
#| fig.align: "center"
tibble(
  eta = seq(-4, 4, length.out = 500),
  logistic = 1 / (1 + exp(-eta)),
  probit = pnorm(eta)
) %>%
  pivot_longer(-eta, names_to = "link", values_to = "p") %>%
  ggplot(aes(x = eta, y = p, color = link)) +
  geom_line(size = 1.2) +
  scale_color_manual(values = c("logistic" =FFHSred, "probit" = FFHSpurp)) +
  labs(
    title = "Logistische vs. Probit-Linkfunktion",
    subtitle = "Transformation von Œ∑ in Wahrscheinlichkeit p",
    x = expression(eta),
    y = "p",
    color = "Linkfunktion"
  ) +
  theme_minimal(base_size = 16)
```

]
]



???

* Beide sind **streng monoton steigend**
* Beide transformieren \(\eta \in (-\infty, \infty)\) ‚Üí \(p \in (0, 1)\)
* Logit = logistische Verteilung ‚Üí schwerere Tails
* Probit = Normalverteilung ‚Üí st√§rker zentriert
* **In der Praxis oft sehr √§hnliche Ergebnisse**

* Logistische Linkfunktion $p =\frac{1}{1+e^{-\eta}}$

mit einer Wahrscheinlichkeit $p$:

$$p =\frac{1}{1+e^{-\eta}}= \frac{1}{1 + e^{-(\beta_0 + \beta_1 x+ \dots + \beta_k\cdot x_k)}}$$

- Transformiert Werte aus $(-\infty, \infty)$ in den Bereich $[0, 1]$
- Ideal f√ºr **bin√§re abh√§ngige Variablen**
- **Inverse** zur logit-Linkfunktion im GLM



```{r}
#| echo: false
#| message: false
#| warning: false
#| out.width: "60%"
#| fig.align: "center"
tibble(
  eta = seq(-10, 10, by = 0.1),
  p = 1 / (1 + exp(-eta))
) %>%
  ggplot(aes(x = eta, y = p)) +
  geom_line(color = "steelblue", size = 1.5) +
  labs(
    title = "Logistische Funktion",
    subtitle = "Transformiert linearen Pr√§diktor Œ∑ in Wahrscheinlichkeit p",
    x = expression(eta == beta[0] + beta[1]*x),
    y = "p"
  ) +
  theme_minimal(base_size = 16)
```





---
class: left

.blockquote[Generalisiertes lineares Modell (`glm()`)]

## Annahmen der logistischen Regression

1. Linearit√§t im Logit:
  * Die Log-Odds m√ºssen linear mit den unabh√§ngigen Variablen zusammenh√§ngen
  * Test: Box-Tidwell-Transformation oder Plots der Logits gegen die Pr√§diktoren
2. Unabh√§ngigkeit der Beobachtungen: Keine Autokorrelation (analog lineare Regression)
3. Keine perfekte Multikollinearit√§t
    * Wie bei linearer Regression, aber VIF-Werte m√ºssen mit Vorsicht interpretiert werden
    * VIF √ºber 2.5 kann bereits problematisch sein (strenger als bei linearer Regression)
4. Ausreichend gro√üe Stichprobe: Als Faustegel mindestens 10 F√§lle der selteneren Kategorie pro Pr√§diktor
5. Keine extremen Ausrei√üer oder einflussreichen F√§lle (Cook's Distance und Leverage-Werte wie bei linearer Regression)






---
class: left

.blockquote[Generalisiertes lineares Modell (`glm()`)]

## Vergleich Lineare vs. Logistische Regression

| **Aspekt**              | **Lineare Regression**                                           | **Logistische Regression**                                                |
|-------------------------|------------------------------------------------------------------|---------------------------------------------------------------------------|
| **Abh√§ngige Variable**  | Metrisch, kontinuierlich                                         | Bin√§r (0/1)                                                               |
| **Funktionsform**       | $Y = \beta_0 + \beta_1 X + \varepsilon$                          | $\eta = \beta_0 + \beta_1 X = \log\left(\frac{p}{1-p}\right)$                    |
| **Interpretation**      | Direkte √Ñnderung in $Y$                                          | √Ñnderung in Log-Odds bzw. Wahrscheinlichkeit                             |
| **Sch√§tzmethode**       | OLS (Kleinste Quadrate)                                          | Maximum Likelihood                                                       |
| **Annahmen**            | Linearit√§t, Normalverteilung, Homoskedastizit√§t                 | Keine Normalverteilung n√∂tig                                             |
| **Vorhersagen**         | Kontinuierliche Werte                                            | Wahrscheinlichkeiten im Bereich [0,1]                                    |

‚ö†Ô∏è Bei bin√§ren AV ist lineare Regression konzeptionell ungeeignet!

???

Die logistische Regression modelliert die Log-Odds (also den Logit) der Wahrscheinlichkeit $p$ als lineare Funktion der Pr√§diktoren
$$\eta = \beta_0 + \beta_1 X = \log\left(\frac{p}{1-p}\right)$$


---
class: inverse,center,middle

# Wir brauchen eine Pause.

---

background-image: url("http://bit.ly/cs631-donkey")
background-size: 80%



---
class: inverse, center, middle

## Logistische Regression:

.blockquote[Anwendungsgebiete]

.blockquote[Modellierung und Umsetzung in R]


.blockquote[Interpretation]



---
class: left

.blockquote[Anwendungsgebiete]

## Einsatzbereiche der logistischen Regression

.panelset[
.panel[.panel-name[Marketing]
### Marketing & Kundenverhalten

- **Kaufentscheidungen vorhersagen**
  - Wahrscheinlichkeit f√ºr Conversion (Kauf/kein Kauf)
  - Einfluss von Preisgestaltung, Werbung, Demografie

- **Kundensegmentierung**
  - Wahrscheinlichkeit f√ºr Zugeh√∂rigkeit zu einem Segment
  - Basis f√ºr personalisiertes Marketing

- **Churn-Prognose**
  - Wahrscheinlichkeit, dass Kunde abwandert
  - Gezielte Retention-Ma√ünahmen erm√∂glichen
]
.panel[.panel-name[Finance]
### Finanzwesen

- **Kreditw√ºrdigkeit**
  - Wahrscheinlichkeit f√ºr Kreditausfall (Scorecard-Modelle)
  - Regulatorische Vorgaben (Basel) erfordern explizite Risikomodellierung

- **Betrugserkennung** 
  - Klassifikation von Transaktionen als verd√§chtig/unbedenklich
  - Echtzeitentscheidungen bei Kartentransaktionen

- **Investitionsentscheidungen**
  - Vorhersage von bin√§ren Marktereignissen (z.B. Kurseinbruch ja/nein)
]
.panel[.panel-name[Etc.]

* Medizinische Diagnostik
    - Risikoprognose (Krankheitsausbruch bei bestimmten Risikofaktoren, z.B. Diabetes, Herzerkrankungen)
    - Diagnoseunterst√ºtzung (Bildgebungsverfahren und Klassifikation von Befunden)
    - Therapieerfolg (Vorhersage der Wahrscheinlichkeit eines positiven Behandlungsverlaufs)

* Politik(wissenschaft): Wahlerfolg Partei A vs. Partei B

* Informatik: Spamfilter (ja/nein)

* Geologie: Bergsturz (ja/nein)

* ...
]
]

???

* Wahrscheinlichkeiten helfen bei Ressourcenallokation:

  * Welche Kunden soll man priorisieren?

  * Wann ist ein Kundenverlust wahrscheinlich?

  * Welche Patienten sind Risikof√§lle?


---
class: left

.blockquote[Modellierung und Umsetzung in R]

## Beispiel: Diskriminierung in Bewerbungsverfahren


```{r}
#| echo: false
#| out.width: "100%"
#| fig.align: "center"
knitr::include_url("https://www.jstor.org/stable/3592802", height="450px")
```

???

* To evaluate which factors were important, job postings were identified in Boston and Chicago for the study, and **researchers created many fake resumes** to send off to these jobs **to see which would elicit a callback**.

* The researchers enumerated important characteristics, such as years of experience and education details, and they used these characteristics to randomly generate fake resumes. Finally, they **randomly assigned a name to each resume, where the name would imply the applicant‚Äôs sex and race**.

* The first names that were used and randomly assigned in the experiment were selected so that they would **predominantly be recognized as belonging to Black or White** individuals; **other races were not considered** in the study. 

* Da man aber nicht mit absoluter Sicherheit sagen kann, ob ein Name "eindeutig" als *schwarz* oder *wei√ü* wahrgenommen wird, 
  * f√ºhrten die Forschenden eine Umfrage durch. In dieser Umfrage pr√ºften sie, wie die Namen von tats√§chlichen Personen wahrgenommen werden: 
  * Wenn ein Name in der Umfrage nicht klar einer bestimmten Rassenzugeh√∂rigkeit zugeordnet wurde, dann wurde er aus dem Experiment ausgeschlossen.



---
class: left

.blockquote[Modellierung und Umsetzung in R]

## Diskriminierungsbeispiel: Daten

```{r}
#| eval: false
library(openintro)
#Das Paket enth√§lt u.a. die resume-Daten
resume %>% 
  select(firstname,race,gender,received_callback)

```
```{r}
#| echo: false
#| message: false
library(openintro)
library(DT)
resume %>% 
    select(firstname,race,gender, received_callback) %>%    # nur die relevanten Spalten
                #distinct() %>%                       # eindeutige Kombinationen
                arrange(firstname) %>% 
datatable(
          options = list(
             scrollX = TRUE,
             scrollY = "230px",
             scrollCollapse = TRUE,
             autoWidth = FALSE,
             columnDefs = list(list(width = '100px', targets = "_all"))
          ),
          rownames = FALSE,
          width = "100%")
```

.quelle[vgl. `r Cite(bib,"crh_intro_2024")`, Kap. 9 und 26).]


???


---
class: left

.blockquote[Modellierung und Umsetzung in R]

## Diskrimierungsbeispiel: Variablenauswahl


| Variable            | Description                                                                                     |
|---------------------|-------------------------------------------------------------------------------------------------|
| received_callback   | Specifies whether the employer called the applicant following submission of the application.   |
| job_city            | City where the job was located: Boston or Chicago.                                              |
| college_degree      | Indicator for whether the resume listed a college degree.                                       |
| years_experience    | Number of years of experience listed on the resume.                                             |
| honors              | Indicator for the resume listing some sort of honors, e.g. employee of the month.              |
| military            | Indicator for if the resume listed any military experience.                                     |
| race                | Race of the applicant, implied by their first name listed on the resume.                        |


---
class: left

.blockquote[Modellierung und Umsetzung in R]

## Diskrimierungsbeispiel: Explorative Datenanalyse

üìù **Aufgabe:** Interpretiert die Statistik bzw. den Boxplot.

```{r}
#| echo: false
resume %>% 
  janitor::tabyl(received_callback,race) %>% gt()
```


```{r}
#| echo: false
#| out-width: "40%"
#| fig-align: "center"
resume %>% 
  ggplot(aes(x=factor(received_callback),y=years_experience)) + geom_boxplot() +
  labs(x="R√ºckruf erhalten", y="Berufserfahrung (in Jahren)") + theme_light()
```

---
class: left

.blockquote[Modellierung und Umsetzung in R]

## Logistische Regression: Durchf√ºhrung und Interpretation

```{r}
mod_full <- resume %>% 
              select(received_callback, job_city,college_degree,years_experience,honors,military,has_email_address,race,gender) %>% 
              glm(received_callback~., family =  binomial,.) 
library(broom)
mod_full %>% 
  glance()  %>% gt()
```

???

Der Akaike Information Criterion (AIC) ist ein Ma√ü zur Modellbewertung und dient dem Vergleich von statistischen Modellen hinsichtlich ihrer G√ºte und Komplexit√§t.

---
class: left

.blockquote[Modellierung und Umsetzung in R]

## Modellselektion mit dem AIC

* Akaike Information Criterion (AIC) kann analog zum angepassten Bestimmtheitsmass (adj $R^2$) verwendet werden, um die Modellg√ºte zu beurteilen.

```{r}
mod_disc <- resume %>% 
              select(received_callback, job_city,years_experience,honors,military,has_email_address,race,gender) %>% 
              glm(received_callback~., family =  binomial,.) 
mod_disc %>% 
  glance()  %>% gt()
```

‚ö†Ô∏è Ohne `college_degree` sinkt AIC nur minimal 



---
class: left

.blockquote[Modellierung und Umsetzung in R]

## Regressionstabelle mit log-Odds (Koeffizienten)

```{r}
mod_disc %>% 
  tidy() %>% gt()
```
???

Vorhersagen und Residuen mit

```{r}
mod_disc %>% 
  augment() %>% head() %>%  gt()
```

---
class: left

.blockquote[Interpretation der Ergebnisse]

## Logistische Regression: Interpretation der Koeffizienten

.panelset[
.panel[.panel-name[Herausforderung]
```{r}
#| echo: false
#| message: false
#| warning: false
#| out.width: "35%"
#| fig.align: "center"
# Visualisierung der Nicht-Linearit√§t
library(tidyverse)

# Funktion zur Berechnung der Wahrscheinlichkeit
logit_prob <- function(x, beta0 = -3, beta1 = 0.5) {
  eta <- beta0 + beta1 * x
  p <- 1 / (1 + exp(-eta))
  return(p)
}

# Datenpunkte erstellen
x_values <- seq(0, 12, 0.1)
prob_values <- logit_prob(x_values)

# Plot erstellen
ggplot(data = tibble(x = x_values, p = prob_values), aes(x = x, y = p)) +
  geom_line(color = "blue", size = 1.5) +
  geom_point(data = tibble(x = c(2, 4, 6, 8, 10), 
                           p = logit_prob(c(2, 4, 6, 8, 10))),
             size = 3, color = "red") +
  geom_segment(data = tibble(x1 = c(2, 4, 6, 8),
                            x2 = c(4, 6, 8, 10),
                            y1 = logit_prob(c(2, 4, 6, 8)),
                            y2 = logit_prob(c(4, 6, 8, 10))),
               aes(x = x1, xend = x2, y = y1, yend = y2),
               linetype = "dashed", color = "darkgreen") +
  annotate("text", x = 2, y = logit_prob(3) + 0.05, 
           label = "Œîp = 0.12", color = "darkgreen") +
  annotate("text", x = 7, y = logit_prob(8) + 0.05, 
           label = "Œîp = 0.25", color = "darkgreen") +
  labs(x = "Pr√§diktor X", 
       y = "Wahrscheinlichkeit p"#,
    #   title = "Nicht-lineare Beziehung",
       #subtitle = "Gleiche √Ñnderung in X f√ºhrt zu unterschiedlichen √Ñnderungen in p"
  ) +
  theme_minimal(base_size = 16)
```

‚ö†Ô∏è **Nicht-Linearit√§t** in der logistischen Regression: Gleiche √Ñnderung in X f√ºhrt zu unterschiedlichen √Ñnderungen in p!

‚û°Ô∏è Interpretation der Koeffizienten nicht so intuitiv wie bei linearer Regression
]
.panel[.panel-name[Koeffizienten]

$$\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_k X_k$$

- **Interpretation von $\beta_j$**:
  - Einheit: √Ñnderung im Log-Odds wenn $X_j$ um 1 Einheit steigt
  - Vorzeichen: Richtung des Effekts (positiv = erh√∂ht Wahrscheinlichkeit)
  
```{r}
#| echo: false
#| message: false
#| warning: false
#| out-width: "65%"
knitr::kable(
  tibble(
    Koeffizient = c("Œ≤ = 0", "Œ≤ > 0", "Œ≤ < 0"),
    Bedeutung = c("Kein Einfluss auf p", "Positiver Einfluss auf p", "Negativer Einfluss auf p"),
    Odds_Ratio = c("OR = 1", "OR > 1", "OR < 1")
  ),
  format = "html",
  caption = "Interpretation der Koeffizienten"
)
```

‚ö†Ô∏è $\beta$-Werte direkt schwer zu interpretieren!
]
.panel[.panel-name[Odds Ratio]
$$\text{OR} = e^{\beta_j}$$

- **Interpretation**:
  - Multiplikativer Effekt auf das Chancenverh√§ltnis (Odds)
  - OR = 1.5 ‚Üí Die Odds steigen um 50% pro Einheit in $X_j$
  - OR = 0.8 ‚Üí Die Odds sinken um 20% pro Einheit in $X_j$
- **Beispiel:**
```{r}
#| echo: false
#| message: false
#| warning: false
#| out-width: "65%"
knitr::kable(
  tibble(
    Pr√§diktor = c("Alter (Jahre)", "Geschlecht (m=1)", "Einkommen (10K ‚Ç¨)"),
    Koeffizient = c(0.05, 1.20, 0.30),
    Odds_Ratio = c("1.05", "3.32", "1.35"),
    Interpretation = c("5% h√∂here Odds pro Lebensjahr", "3.3-fache Odds f√ºr M√§nner", "35% h√∂here Odds pro 10K ‚Ç¨ Einkommen")
  ),
  format = "html",
  caption = "Beispiel: Odds Ratios und Interpretation"
)
```

‚û°Ô∏è OR = e^Œ≤ macht den Effekt intuitiv verst√§ndlich!
]
]




---
class: left

.blockquote[Interpretation der Ergebnisse]

## Diskriminierungsbeispiel: Wahrscheinlichkeit f√ºr einen R√ºckruf

.panelset[
.panel[.panel-name[log-Odds]
$$
\begin{align*}
\log_e\left(\frac{\hat{p}}{1-\hat{p}}\right) =\ & -2.7162 \\
& - 0.4364 \cdot \text{job_city}_{\text{Chicago}} \\
& + 0.0206 \cdot \text{years_experience} \\
& + 0.7634 \cdot \text{honors} \\
& + 0.3443 \cdot \text{military} \\
& + 0.2221 \cdot \text{email} \\
& + 0.4429 \cdot \text{race}_{\text{White}} \\
& - 0.1959 \cdot \text{sex}_{\text{Male}}
\end{align*}
$$
]
.panel[.panel-name[Beispiel]
* Beispiel: Kandidat f√ºr einen Job in Chicago mit 14 Jahren Berufserfahrung, ohne Auszeichnungen und Milit√§rdienst, mit Email-Adresse und einem Namen, der "weiss klingt":
$$
\begin{align*}
\log_e\left(\frac{\hat{p}}{1-\hat{p}}\right) =\ & -2.7162 \\
& - 0.4364 \cdot 1 \\
& + 0.0206 \cdot 14 \\
& + 0.7634 \cdot 0 \\
& + 0.3443 \cdot 0 \\
& + 0.2221 \cdot 1 \\
& + 0.4429 \cdot 1 \\
& - 0.1959 \cdot 1 \\
= &\ -2.3955
\end{align*}
$$


* Somit betr√§gt die R√ºckruf-Wahrscheinlichkeit:

$$\hat{p}=\frac{e^{-2.3955}}{1+e^{-2.3955}}=0.0835 \implies \approx 8\%$$
]
]




---
class: left

.blockquote[Interpretation der Ergebnisse]

## Marginal Effects - Alternative zur Interpretation

.panelset[
.panel[.panel-name[Konzept]
### Marginal Effects: Intuitive Alternative

- **Definition**: √Ñnderung in der Wahrscheinlichkeit bei √Ñnderung des Pr√§diktors um 1 Einheit
- **Vorteil**: Direkte Aussage zur Wahrscheinlichkeit (statt Log-Odds)
- **Berechnung**: $\frac{\partial p}{\partial X_j} = \beta_j \cdot p \cdot (1-p)$
]
.panel[.panel-name[Visualisierung]
```{r}
#| echo: false
#| message: false
#| warning: false
#| out.width: "40%"
#| fig.align: "center"
# Funktion f√ºr marginale Effekte
marginal_effect <- function(x, beta0 = -3, beta1 = 0.5) {
  p <- logit_prob(x, beta0, beta1)
  me <- beta1 * p * (1-p)
  return(me)
}

# Datenpunkte erstellen
me_values <- marginal_effect(x_values)

# Plot erstellen
ggplot(data = tibble(x = x_values, me = me_values), aes(x = x, y = me)) +
  geom_line(color = "purple", size = 1.5) +
  labs(x = "Pr√§diktor X", 
       y = "Marginaler Effekt",
       title = "Marginaler Effekt in Abh√§ngigkeit von X",
       subtitle = "Maximaler Effekt bei p = 0.5") +
  theme_minimal(base_size = 16)
```

‚ö†Ô∏è Marginale Effekte variieren je nach Auspr√§gung der Pr√§diktoren
]
.panel[.panel-name[Arten]
### Arten von Marginalen Effekten

| Typ | Beschreibung | Verwendung |
|-----|--------------|------------|
| **MEM** (Marginal Effects at Means) | Effekt an den Mittelwerten aller Variablen | F√ºr "durchschnittliche" Aussagen |
| **AME** (Average Marginal Effects) | Durchschnitt der individuellen marginalen Effekte | F√ºr populationsweite Aussagen |
| **MER** (Marginal Effects at Representative values) | Effekt bei spezifischen Werten | F√ºr konkrete Szenarien |

]
.panel[.panel-name[Beispiel AME]
```{r}
#| echo: false
#| message: false
#| warning: false
knitr::kable(
  tibble(
    Pr√§diktor = c("Alter (Jahre)", "Geschlecht (m=1)", "Einkommen (10K ‚Ç¨)"),
    AME = c("0.010", "0.214", "0.058"),
    Interpretation = c("1 Prozentpunkt erh√∂hte Wahrscheinlichkeit pro Lebensjahr", "21.4 Prozentpunkte erh√∂hte Wahrscheinlichkeit f√ºr M√§nner", "5.8 Prozentpunkte erh√∂hte Wahrscheinlichkeit pro 10K ‚Ç¨ Einkommen")
  ),
  format = "html",
  caption = "Average Marginal Effects (AME)"
)
```
‚û°Ô∏è Direkte Aussage zur √Ñnderung der Wahrscheinlichkeit in Prozentpunkten!

]
]

???

* Warum ist das Maximum bei $p=0,5$?
  * Marginaler Effekt: $$\mbox{Marginaler Effekt} = \frac{\partial p(x)}{\partial x}=\beta_1\cdot p(x)\cdot (1-p(x))$$
  * Die Funktion $f(p)=p(1-p)$ ist eine quadratische Funktion, die ein Maximum bei $p=0,5$ hat.
  * Dies zeigt die Ableitung: $$\frac{d}{dp}[p(1‚àíp)]=1‚àí2p
  * Setze die Ableitung gleich 0: $1‚àí2p=0‚áíp=0,5$
  * Extremwert von $p(1‚àíp)$, was wiederum den maximalen marginalen Effekt ergibt (da $\beta_1$  konstant ist).

* Intuition:
  * Bei $p=0.5$ ist die Unsicherheit am gr√∂√üten - die Modellwahrscheinlichkeit liegt "in der Schwebe", also mittig zwischen 0 und 1.
  * Hier reagiert die Wahrscheinlichkeit am empfindlichsten auf Ver√§nderungen von $X$.
  * Bei sehr kleinen oder gro√üen $p$ (also nahe 0 oder 1) ist das Modell "sicher", und √Ñnderungen in $X$ √§ndern $p$ kaum noch ‚Üí marginaler Effekt fast 0. 







---
class: inverse,center,middle

# Wir brauchen eine Pause.

---

background-image: url("http://bit.ly/cs631-donkey")
background-size: 80%





---
class: left

## Quellenverzeichnis

.ref-slide[
```{r, results='asis', echo=FALSE, warning=FALSE}
PrintBibliography(bib)
```
]
